#!/usr/bin/env python3
"""
üîå N8N CONNECTOR - –ú–æ–¥—É–ª—å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å N8N

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å N8N —á–µ—Ä–µ–∑:
- REST API –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è workflow'–∞–º–∏
- SSH –¥–ª—è –ø—Ä—è–º–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ —Å–µ—Ä–≤–µ—Ä—É
- PostgreSQL –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö
- Webhook endpoints –¥–ª—è real-time —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π

–ê–≤—Ç–æ—Ä: AI Assistant
–î–∞—Ç–∞: 2025-10-02
–í–µ—Ä—Å–∏—è: 1.0
"""

import asyncio
import json
import logging
import subprocess
import time
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
import aiohttp
import asyncpg
from pathlib import Path

logger = logging.getLogger(__name__)

@dataclass
class WorkflowInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ workflow"""
    id: str
    name: str
    active: bool
    nodes_count: int
    connections_count: int
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None

@dataclass
class ExecutionInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏"""
    id: str
    workflow_id: str
    status: str
    finished: bool
    started_at: Optional[datetime] = None
    stopped_at: Optional[datetime] = None
    execution_time: Optional[float] = None
    error: Optional[str] = None

@dataclass
class NodeInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –Ω–æ–¥–µ"""
    id: str
    name: str
    type: str
    parameters: Dict[str, Any]
    position: List[int]
    credentials: Optional[Dict[str, Any]] = None

class N8NConnector:
    """
    –ö–æ–Ω–Ω–µ–∫—Ç–æ—Ä –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å N8N
    
    –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —á–µ—Ä–µ–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫–∞–Ω–∞–ª—ã:
    - REST API –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
    - SSH –¥–ª—è —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –∫–æ–º–∞–Ω–¥
    - PostgreSQL –¥–ª—è –ø—Ä—è–º–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –¥–∞–Ω–Ω—ã–º
    """
    
    def __init__(self, api_url: str = None, ssh_host: str = None, db_config: Dict = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω–Ω–µ–∫—Ç–æ—Ä–∞"""
        self.api_url = api_url or "https://mayersn8n.duckdns.org"
        self.ssh_host = ssh_host or "root@178.156.142.35"
        self.db_config = db_config or {
            "host": "178.156.142.35",
            "port": 5432,
            "database": "n8n",
            "user": "n8n"
        }
        
        # HTTP —Å–µ—Å—Å–∏—è –¥–ª—è API –∑–∞–ø—Ä–æ—Å–æ–≤
        self.session: Optional[aiohttp.ClientSession] = None
        
        # PostgreSQL –ø—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        self.db_pool: Optional[asyncpg.Pool] = None
        
        # –ö—ç—à –¥–ª—è —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        self._workflow_cache: Dict[str, WorkflowInfo] = {}
        self._cache_ttl = 300  # 5 –º–∏–Ω—É—Ç
        self._last_cache_update = 0
        
        logger.info("üîå N8N Connector initialized")
    
    async def __aenter__(self):
        """Async context manager entry"""
        await self.connect()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        await self.close()
    
    async def connect(self):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å N8N"""
        try:
            # HTTP —Å–µ—Å—Å–∏—è
            timeout = aiohttp.ClientTimeout(total=30)
            self.session = aiohttp.ClientSession(timeout=timeout)
            
            # PostgreSQL –ø—É–ª
            self.db_pool = await asyncpg.create_pool(
                host=self.db_config["host"],
                port=self.db_config["port"],
                database=self.db_config["database"],
                user=self.db_config["user"],
                min_size=1,
                max_size=10
            )
            
            logger.info("‚úÖ N8N Connector connected successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to connect N8N Connector: {e}")
            raise
    
    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        try:
            if self.session:
                await self.session.close()
            
            if self.db_pool:
                await self.db_pool.close()
            
            logger.info("üîå N8N Connector closed")
            
        except Exception as e:
            logger.error(f"‚ùå Error closing N8N Connector: {e}")
    
    async def health_check(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∑–¥–æ—Ä–æ–≤—å–µ N8N —Å–µ—Ä–≤–∏—Å–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ SSH —á—Ç–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –∑–∞–ø—É—â–µ–Ω
            result = await self._run_ssh_command("docker ps | grep n8n | grep -v Exited")
            
            if result["success"] and result["stdout"]:
                logger.debug("‚úÖ N8N container is running")
                return True
            else:
                logger.warning("‚ùå N8N container is not running")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå N8N health check failed: {e}")
            return False
    
    async def database_health_check(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∑–¥–æ—Ä–æ–≤—å–µ PostgreSQL"""
        try:
            result = await self._run_ssh_command("docker exec root-db-1 pg_isready -U n8n")
            
            if result["success"] and "accepting" in result["stdout"]:
                logger.debug("‚úÖ PostgreSQL is healthy")
                return True
            else:
                logger.warning("‚ùå PostgreSQL is not healthy")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå PostgreSQL health check failed: {e}")
            return False
    
    async def mcp_server_health_check(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∑–¥–æ—Ä–æ–≤—å–µ MCP —Å–µ—Ä–≤–µ—Ä–∞"""
        try:
            result = await self._run_ssh_command("curl -s -o /dev/null -w '%{http_code}' http://localhost:4123/api/ping")
            
            if result["success"] and result["stdout"] == "200":
                logger.debug("‚úÖ MCP Server is healthy")
                return True
            else:
                logger.warning("‚ùå MCP Server is not healthy")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå MCP Server health check failed: {e}")
            return False
    
    async def get_workflows(self, active_only: bool = False) -> List[WorkflowInfo]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ workflow'–æ–≤"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            if self._is_cache_valid():
                workflows = list(self._workflow_cache.values())
                if active_only:
                    workflows = [w for w in workflows if w.active]
                return workflows
            
            # –ó–∞–ø—Ä–æ—Å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
            query = """
            SELECT id, name, active, 
                   LENGTH(nodes::text) as nodes_size,
                   LENGTH(connections::text) as connections_size,
                   "createdAt", "updatedAt"
            FROM workflow_entity
            """
            
            if active_only:
                query += " WHERE active = true"
            
            query += " ORDER BY \"updatedAt\" DESC"
            
            async with self.db_pool.acquire() as conn:
                rows = await conn.fetch(query)
            
            workflows = []
            for row in rows:
                workflow = WorkflowInfo(
                    id=row["id"],
                    name=row["name"],
                    active=row["active"],
                    nodes_count=row["nodes_size"] // 100,  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
                    connections_count=row["connections_size"] // 50,
                    created_at=row["createdAt"],
                    updated_at=row["updatedAt"]
                )
                workflows.append(workflow)
                self._workflow_cache[workflow.id] = workflow
            
            self._last_cache_update = time.time()
            logger.debug(f"üìã Retrieved {len(workflows)} workflows")
            
            return workflows
            
        except Exception as e:
            logger.error(f"‚ùå Failed to get workflows: {e}")
            return []
    
    async def get_workflow_by_id(self, workflow_id: str) -> Optional[WorkflowInfo]:
        """–ü–æ–ª—É—á–∞–µ—Ç workflow –ø–æ ID"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            if workflow_id in self._workflow_cache and self._is_cache_valid():
                return self._workflow_cache[workflow_id]
            
            query = """
            SELECT id, name, active, 
                   LENGTH(nodes::text) as nodes_size,
                   LENGTH(connections::text) as connections_size,
                   "createdAt", "updatedAt"
            FROM workflow_entity
            WHERE id = $1
            """
            
            async with self.db_pool.acquire() as conn:
                row = await conn.fetchrow(query, workflow_id)
            
            if row:
                workflow = WorkflowInfo(
                    id=row["id"],
                    name=row["name"],
                    active=row["active"],
                    nodes_count=row["nodes_size"] // 100,
                    connections_count=row["connections_size"] // 50,
                    created_at=row["createdAt"],
                    updated_at=row["updatedAt"]
                )
                self._workflow_cache[workflow_id] = workflow
                return workflow
            
            return None
            
        except Exception as e:
            logger.error(f"‚ùå Failed to get workflow {workflow_id}: {e}")
            return None
    
    async def get_workflow_nodes(self, workflow_id: str) -> List[NodeInfo]:
        """–ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–¥—ã workflow'–∞"""
        try:
            query = "SELECT nodes FROM workflow_entity WHERE id = $1"
            
            async with self.db_pool.acquire() as conn:
                row = await conn.fetchrow(query, workflow_id)
            
            if not row or not row["nodes"]:
                return []
            
            nodes_data = json.loads(row["nodes"])
            nodes = []
            
            for node_data in nodes_data:
                node = NodeInfo(
                    id=node_data.get("id", ""),
                    name=node_data.get("name", ""),
                    type=node_data.get("type", ""),
                    parameters=node_data.get("parameters", {}),
                    position=node_data.get("position", [0, 0]),
                    credentials=node_data.get("credentials")
                )
                nodes.append(node)
            
            logger.debug(f"üì¶ Retrieved {len(nodes)} nodes for workflow {workflow_id}")
            return nodes
            
        except Exception as e:
            logger.error(f"‚ùå Failed to get workflow nodes: {e}")
            return []
    
    async def update_workflow_nodes(self, workflow_id: str, nodes: List[NodeInfo]) -> bool:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –Ω–æ–¥—ã workflow'–∞"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ–¥—ã –≤ JSON —Ñ–æ—Ä–º–∞—Ç
            nodes_data = []
            for node in nodes:
                node_dict = {
                    "id": node.id,
                    "name": node.name,
                    "type": node.type,
                    "parameters": node.parameters,
                    "position": node.position
                }
                if node.credentials:
                    node_dict["credentials"] = node.credentials
                nodes_data.append(node_dict)
            
            nodes_json = json.dumps(nodes_data, ensure_ascii=False)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
            query = """
            UPDATE workflow_entity 
            SET nodes = $1, "updatedAt" = NOW()
            WHERE id = $2
            """
            
            async with self.db_pool.acquire() as conn:
                result = await conn.execute(query, nodes_json, workflow_id)
            
            if result == "UPDATE 1":
                # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à
                if workflow_id in self._workflow_cache:
                    del self._workflow_cache[workflow_id]
                
                # –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º N8N –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
                await self._restart_n8n()
                
                logger.info(f"‚úÖ Updated nodes for workflow {workflow_id}")
                return True
            else:
                logger.error(f"‚ùå Failed to update workflow {workflow_id}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Failed to update workflow nodes: {e}")
            return False
    
    async def create_workflow(self, name: str, nodes: List[NodeInfo], connections: Dict = None) -> Optional[str]:
        """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π workflow"""
        try:
            workflow_id = str(uuid.uuid4())
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            nodes_data = []
            for node in nodes:
                node_dict = {
                    "id": node.id,
                    "name": node.name,
                    "type": node.type,
                    "parameters": node.parameters,
                    "position": node.position
                }
                if node.credentials:
                    node_dict["credentials"] = node.credentials
                nodes_data.append(node_dict)
            
            nodes_json = json.dumps(nodes_data, ensure_ascii=False)
            connections_json = json.dumps(connections or {}, ensure_ascii=False)
            
            # –°–æ–∑–¥–∞–µ–º workflow –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
            query = """
            INSERT INTO workflow_entity (
                id, name, nodes, connections, active, "createdAt", "updatedAt"
            ) VALUES (
                $1, $2, $3, $4, false, NOW(), NOW()
            )
            """
            
            async with self.db_pool.acquire() as conn:
                await conn.execute(query, workflow_id, name, nodes_json, connections_json)
            
            logger.info(f"‚úÖ Created workflow {workflow_id}: {name}")
            return workflow_id
            
        except Exception as e:
            logger.error(f"‚ùå Failed to create workflow: {e}")
            return None
    
    async def activate_workflow(self, workflow_id: str) -> bool:
        """–ê–∫—Ç–∏–≤–∏—Ä—É–µ—Ç workflow"""
        try:
            query = """
            UPDATE workflow_entity 
            SET active = true, "updatedAt" = NOW()
            WHERE id = $1
            """
            
            async with self.db_pool.acquire() as conn:
                result = await conn.execute(query, workflow_id)
            
            if result == "UPDATE 1":
                # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à
                if workflow_id in self._workflow_cache:
                    del self._workflow_cache[workflow_id]
                
                logger.info(f"‚úÖ Activated workflow {workflow_id}")
                return True
            else:
                logger.error(f"‚ùå Failed to activate workflow {workflow_id}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Failed to activate workflow: {e}")
            return False
    
    async def deactivate_workflow(self, workflow_id: str) -> bool:
        """–î–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ—Ç workflow"""
        try:
            query = """
            UPDATE workflow_entity 
            SET active = false, "updatedAt" = NOW()
            WHERE id = $1
            """
            
            async with self.db_pool.acquire() as conn:
                result = await conn.execute(query, workflow_id)
            
            if result == "UPDATE 1":
                # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à
                if workflow_id in self._workflow_cache:
                    del self._workflow_cache[workflow_id]
                
                logger.info(f"‚úÖ Deactivated workflow {workflow_id}")
                return True
            else:
                logger.error(f"‚ùå Failed to deactivate workflow {workflow_id}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Failed to deactivate workflow: {e}")
            return False
    
    async def execute_workflow(self, workflow_id: str, input_data: Dict = None) -> Optional[str]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç workflow –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç execution ID"""
        try:
            execution_id = str(uuid.uuid4())
            
            # –°–æ–∑–¥–∞–µ–º execution –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
            query = """
            INSERT INTO execution_entity (
                id, "workflowId", mode, finished, status, "startedAt", "createdAt"
            ) VALUES (
                $1, $2, 'manual', false, 'running', NOW(), NOW()
            )
            """
            
            async with self.db_pool.acquire() as conn:
                await conn.execute(query, execution_id, workflow_id)
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º workflow —á–µ—Ä–µ–∑ N8N CLI
            input_json = json.dumps(input_data or {}, ensure_ascii=False)
            temp_file = f"/tmp/input_{execution_id}.json"
            
            # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            write_cmd = f"cat > {temp_file} << 'EOF'\n{input_json}\nEOF"
            await self._run_ssh_command(write_cmd)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º workflow
            execute_cmd = f"docker exec root-n8n-1 n8n execute:workflow --id={workflow_id} --input={temp_file}"
            result = await self._run_ssh_command(execute_cmd, timeout=300)
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            await self._run_ssh_command(f"rm -f {temp_file}")
            
            if result["success"]:
                logger.info(f"‚úÖ Started execution {execution_id} for workflow {workflow_id}")
                return execution_id
            else:
                logger.error(f"‚ùå Failed to execute workflow {workflow_id}: {result['stderr']}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Failed to execute workflow: {e}")
            return None
    
    async def get_execution_status(self, execution_id: str) -> Optional[ExecutionInfo]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
        try:
            query = """
            SELECT id, "workflowId", status, finished, "startedAt", "stoppedAt"
            FROM execution_entity
            WHERE id = $1
            """
            
            async with self.db_pool.acquire() as conn:
                row = await conn.fetchrow(query, execution_id)
            
            if row:
                execution_time = None
                if row["startedAt"] and row["stoppedAt"]:
                    execution_time = (row["stoppedAt"] - row["startedAt"]).total_seconds()
                
                execution = ExecutionInfo(
                    id=row["id"],
                    workflow_id=row["workflowId"],
                    status=row["status"],
                    finished=row["finished"],
                    started_at=row["startedAt"],
                    stopped_at=row["stoppedAt"],
                    execution_time=execution_time
                )
                
                return execution
            
            return None
            
        except Exception as e:
            logger.error(f"‚ùå Failed to get execution status: {e}")
            return None
    
    async def get_execution_errors(self, execution_id: str) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ—à–∏–±–∫–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
        try:
            query = """
            SELECT data
            FROM execution_data
            WHERE "executionId" = $1
            """
            
            async with self.db_pool.acquire() as conn:
                row = await conn.fetchrow(query, execution_id)
            
            if not row or not row["data"]:
                return []
            
            data = json.loads(row["data"])
            errors = []
            
            if "resultData" in data and "runData" in data["resultData"]:
                run_data = data["resultData"]["runData"]
                
                for node_name, node_results in run_data.items():
                    if isinstance(node_results, list) and len(node_results) > 0:
                        node_result = node_results[0]
                        
                        if "error" in node_result:
                            errors.append({
                                "node": node_name,
                                "error": node_result["error"],
                                "execution_id": execution_id
                            })
            
            return errors
            
        except Exception as e:
            logger.error(f"‚ùå Failed to get execution errors: {e}")
            return []
    
    async def get_recent_executions(self, limit: int = 50) -> List[ExecutionInfo]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
        try:
            query = """
            SELECT id, "workflowId", status, finished, "startedAt", "stoppedAt"
            FROM execution_entity
            ORDER BY "startedAt" DESC
            LIMIT $1
            """
            
            async with self.db_pool.acquire() as conn:
                rows = await conn.fetch(query, limit)
            
            executions = []
            for row in rows:
                execution_time = None
                if row["startedAt"] and row["stoppedAt"]:
                    execution_time = (row["stoppedAt"] - row["startedAt"]).total_seconds()
                
                execution = ExecutionInfo(
                    id=row["id"],
                    workflow_id=row["workflowId"],
                    status=row["status"],
                    finished=row["finished"],
                    started_at=row["startedAt"],
                    stopped_at=row["stoppedAt"],
                    execution_time=execution_time
                )
                executions.append(execution)
            
            return executions
            
        except Exception as e:
            logger.error(f"‚ùå Failed to get recent executions: {e}")
            return []
    
    async def _run_ssh_command(self, command: str, timeout: int = 30) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç SSH –∫–æ–º–∞–Ω–¥—É –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ"""
        try:
            process = await asyncio.create_subprocess_exec(
                "ssh", self.ssh_host, command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await asyncio.wait_for(
                process.communicate(),
                timeout=timeout
            )
            
            return {
                "success": process.returncode == 0,
                "stdout": stdout.decode().strip(),
                "stderr": stderr.decode().strip(),
                "returncode": process.returncode
            }
            
        except asyncio.TimeoutError:
            logger.error(f"‚ùå SSH command timeout: {command}")
            return {"success": False, "stdout": "", "stderr": "Timeout", "returncode": -1}
        except Exception as e:
            logger.error(f"‚ùå SSH command error: {e}")
            return {"success": False, "stdout": "", "stderr": str(e), "returncode": -1}
    
    async def _restart_n8n(self):
        """–ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ—Ç N8N –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        try:
            logger.info("üîÑ Restarting N8N...")
            result = await self._run_ssh_command("docker restart root-n8n-1")
            
            if result["success"]:
                # –ñ–¥–µ–º –∑–∞–ø—É—Å–∫–∞
                await asyncio.sleep(15)
                logger.info("‚úÖ N8N restarted successfully")
            else:
                logger.error(f"‚ùå Failed to restart N8N: {result['stderr']}")
                
        except Exception as e:
            logger.error(f"‚ùå Error restarting N8N: {e}")
    
    def _is_cache_valid(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∫—ç—à–∞"""
        return (time.time() - self._last_cache_update) < self._cache_ttl
    
    def clear_cache(self):
        """–û—á–∏—â–∞–µ—Ç –∫—ç—à"""
        self._workflow_cache.clear()
        self._last_cache_update = 0
        logger.debug("üóëÔ∏è Cache cleared")

# –£—Ç–∏–ª–∏—Ç–∞—Ä–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å N8N

async def create_test_workflow() -> Optional[str]:
    """–°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–π workflow –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
    async with N8NConnector() as connector:
        # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç–æ–≤—ã–π workflow
        nodes = [
            NodeInfo(
                id="manual_trigger",
                name="Manual Trigger",
                type="n8n-nodes-base.manualTrigger",
                parameters={},
                position=[100, 100]
            ),
            NodeInfo(
                id="set_node",
                name="Set Test Data",
                type="n8n-nodes-base.set",
                parameters={
                    "values": {
                        "string": [
                            {
                                "name": "message",
                                "value": "Test workflow executed successfully"
                            }
                        ]
                    }
                },
                position=[300, 100]
            )
        ]
        
        connections = {
            "manual_trigger": {
                "main": [[{
                    "node": "set_node",
                    "type": "main",
                    "index": 0
                }]]
            }
        }
        
        workflow_id = await connector.create_workflow(
            name="üß™ Test Workflow - Autonomous System",
            nodes=nodes,
            connections=connections
        )
        
        if workflow_id:
            await connector.activate_workflow(workflow_id)
        
        return workflow_id

async def cleanup_test_workflows():
    """–û—á–∏—â–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ workflow'—ã"""
    async with N8NConnector() as connector:
        workflows = await connector.get_workflows()
        
        for workflow in workflows:
            if "Test Workflow" in workflow.name or "üß™" in workflow.name:
                logger.info(f"üóëÔ∏è Cleaning up test workflow: {workflow.name}")
                # –î–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ–º workflow
                await connector.deactivate_workflow(workflow.id)
