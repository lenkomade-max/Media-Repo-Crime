import fs from "fs/promises";
import path from "path";
import { execa } from "execa";
import { log } from "../logger.js";

/**
 * –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Whisper CLI –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ
 */
async function checkWhisperAvailability(): Promise<{ available: boolean; version?: string; error?: string }> {
  try {
    const { stdout } = await execa("whisper", ["--version"], { timeout: 5000 });
    return { available: true, version: stdout.trim() };
  } catch (error: any) {
    return { 
      available: false, 
      error: error.code === 'ENOENT' ? 'Whisper CLI not found' : error.message 
    };
  }
}

/**
 * –ó–∞–ø—É—Å–∫–∞–µ—Ç whisper CLI –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∞—É–¥–∏–æ –≤ .srt
 * –¢—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π whisper (`pip install -U openai-whisper`) –∏ ffmpeg.
 * –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ .srt
 */
export async function transcribeWithWhisper(audioPath: string, outDir: string, model = "base"): Promise<string> {
  const startTime = Date.now();
  
  try {
    log.info(`üé§ Whisper Start: model=${model}, audio=${path.basename(audioPath)}`);
    
    // –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Whisper
    const availability = await checkWhisperAvailability();
    if (!availability.available) {
      throw new Error(`Whisper CLI unavailable: ${availability.error}`);
    }
    
    log.info(`üé§ Whisper available: ${availability.version}`);
    
    // –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞
    const audioExists = await fs.access(audioPath).then(() => true).catch(() => false);
    if (!audioExists) {
      throw new Error(`Audio file not found: ${audioPath}`);
    }
    
    const audioStats = await fs.stat(audioPath);
    log.info(`üé§ Audio file: ${audioPath} (${audioStats.size} bytes)`);
    
    // –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    const validModels = ["tiny", "base", "small", "medium", "large", "large-v1", "large-v2", "large-v3"];
    if (!validModels.includes(model)) {
      log.warn(`üé§ Invalid model '${model}', using 'base' instead`);
      model = "base";
    }
    
    // Whisper –∫–æ–º–∞–Ω–¥–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –ª–æ–≥–∞–º–∏
    const cmd = ["whisper", audioPath, "--model", model, "--output_format", "srt", "--output_dir", outDir];
    log.info(`üé§ Whisper command: ${cmd.join(" ")}`);
    
    const { stdout, stderr } = await execa("whisper", [audioPath, "--model", model, "--output_format", "srt", "--output_dir", outDir], {
      stdio: "pipe",
      timeout: 120000, // 2 –º–∏–Ω—É—Ç—ã —Ç–∞–π–º–∞—É—Ç
    });
    
    if (stderr) {
      log.info(`üé§ Whisper stderr: ${stderr}`);
    }
    if (stdout) {
      log.info(`üé§ Whisper stdout: ${stdout}`);
    }
    
    // –ü–æ–∏—Å–∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–≥–æ SRT —Ñ–∞–π–ª–∞
    const base = path.basename(audioPath).replace(/\.[^.]+$/, "");
    const srtPath = path.join(outDir, `${base}.srt`);
    
    // –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è SRT
    const srtExists = await fs.access(srtPath).then(() => true).catch(() => false);
    if (!srtExists) {
      throw new Error(`SRT file was not created: ${srtPath}`);
    }
    
    const srtStats = await fs.stat(srtPath);
    const srtContent = await fs.readFile(srtPath, "utf8");
    const subtitleCount = (srtContent.match(/^\d+$/gm) || []).length;
    
    log.info(`üé§ Whisper success: ${srtPath} (${srtStats.size} bytes, ${subtitleCount} subtitles, ${Date.now() - startTime}ms)`);
    
    return srtPath;
    
  } catch (error: any) {
    log.error(`üé§ Whisper Error (${Date.now() - startTime}ms): ${error.message}`);
    
    // –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –æ—à–∏–±–∫–∏
    if (error.code === 'ENOENT') {
      log.error(`üé§ Whisper CLI not found. Install: pip3 install openai-whisper`);
    } else if (error.timedOut) {
      log.error(`üé§ Whisper timeout after ${error.timeout}ms`);
    } else if (error.stderr) {
      log.error(`üé§ Whisper stderr: ${error.stderr}`);
    }
    
    throw new Error(`Whisper transcription failed: ${error.message}`);
  }
}
